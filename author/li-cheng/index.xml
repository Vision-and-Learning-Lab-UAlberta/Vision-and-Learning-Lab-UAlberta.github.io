<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Li Cheng | Vision and Learning Lab @ UAlberta</title>
    <link>https://vision-and-learning-lab-ualberta.github.io/author/li-cheng/</link>
      <atom:link href="https://vision-and-learning-lab-ualberta.github.io/author/li-cheng/index.xml" rel="self" type="application/rss+xml" />
    <description>Li Cheng</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© Vision and Learning Lab, Univerity of Alberta 2024</copyright><lastBuildDate>Tue, 16 Jan 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://vision-and-learning-lab-ualberta.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Li Cheng</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/author/li-cheng/</link>
    </image>
    
    <item>
      <title>Generative Human Motion Stylization in Latent Space</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/chuan-iclr-2024/</link>
      <pubDate>Tue, 16 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/chuan-iclr-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Generative Human Motion Stylization in Latent Space &#34; is accepted by International Conference on Learning Representations (ICLR) 2024!</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/chuan_iclr_2024/</link>
      <pubDate>Tue, 16 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/chuan_iclr_2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DVSOD: RGB-D Video Salient Object Detection</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/jingjing-et-al-nips-23/</link>
      <pubDate>Thu, 21 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/jingjing-et-al-nips-23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SemanticRT: A Large-Scale Dataset and Method for Robust Semantic Segmentation in Multispectral Images</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/wei-et-al-acm-23/</link>
      <pubDate>Tue, 01 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/wei-et-al-acm-23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our CVPR 2023 workshop paper &#34;Segment Anything Is Not Always Perfect: An Investigation of SAM on Different Real-world Applications&#34; has won the most insightful paper award presented by the 1st Workshop on Vision-based InduStrial InspectiON!</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/wei_cvprws_2023/</link>
      <pubDate>Fri, 28 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/wei_cvprws_2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Segment Anything Is Not Always Perfect: An Investigation of SAM on Different Real-world Applications</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/wei-et-al-cvprws-23/</link>
      <pubDate>Sat, 01 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/wei-et-al-cvprws-23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>BigNeuron: A resource to benchmark and predict best-performing algorithms for automated reconstruction of neuronal morphology</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/li-et-al-nature-23/</link>
      <pubDate>Thu, 11 May 2023 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/li-et-al-nature-23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multispectral Video Semantic Segmentation: A Benchmark Dataset and Baseline</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/wei-et-al-cvpr-23/</link>
      <pubDate>Wed, 01 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/wei-et-al-cvpr-23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Multispectral Video Semantic Segmentation: A Benchmark Dataset and Baseline&#34; is accepted by IEEE Conference on Computer Vision and Pattern Recognition 2023!</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/wei_cvpr_2023/</link>
      <pubDate>Tue, 28 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/wei_cvpr_2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Delving into Calibrated Depth for Accurate RGB-D Salient Object Detection</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/jingjing-et-al-ijcv-23/</link>
      <pubDate>Mon, 13 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/jingjing-et-al-ijcv-23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Snipper: A Spatiotemporal Transformer for Simultaneous Multi-Person 3D Pose Estimation Tracking and Forecasting on a Video Snippet</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/shihao-et-al-tcsvt-23/</link>
      <pubDate>Mon, 13 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/shihao-et-al-tcsvt-23/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Our paper &#34;Delving into Calibrated Depth for Accurate RGB-D Salient Object Detection&#34; is accepted by Interactional Journal of Computer Vision (IJCV) 2023!</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/jingjing_ijcv2023/</link>
      <pubDate>Fri, 09 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/jingjing_ijcv2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Resource-Efficient Medical Image Analysis</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/li-et-al-springer-22/</link>
      <pubDate>Thu, 22 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/li-et-al-springer-22/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Object Wake-up: 3D Object Rigging from a Single Image.</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/ji-et-al-objwu-eccv-2022/</link>
      <pubDate>Fri, 15 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/ji-et-al-objwu-eccv-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Object Wake-up: 3D Object Rigging from a Single Image&#34; is accepted by European Conference on Computer Vision (ECCV) 2022!</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/ji_eccv_2022/</link>
      <pubDate>Fri, 15 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/ji_eccv_2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts.&#34; is accepted by European Conference on Computer Vision (ECCV) 2022!</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/chuan_eccv_2022/</link>
      <pubDate>Fri, 15 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/chuan_eccv_2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts.</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/chuan-tm2t-eccv-2022/</link>
      <pubDate>Fri, 15 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/chuan-tm2t-eccv-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Human Pose and Shape Estimation from Single Polarization Images</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/shihao-et-al-tmm-2022/</link>
      <pubDate>Fri, 25 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/shihao-et-al-tmm-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Contrastive Learning for Unsupervised Video Highlight Detection&#34; is accepted by IEEE Conference on Computer Vision and Pattern Recognition(CVPR) 2022!</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/tk_cvpr_2022/</link>
      <pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/tk_cvpr_2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Exploring Denoised Cross-video Contrast for Weakly-supervised Temporal Action Localization&#34; is accepted by IEEE Conference on Computer Vision and Pattern Recognition(CVPR) 2022!</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/jingjing_cvpr_2022/</link>
      <pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/jingjing_cvpr_2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Generating Diverse and Natural 3D Human Motions from Text&#34; is accepted by IEEE Conference on Computer Vision and Pattern Recognition(CVPR) 2022!</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/chuan_cvpr_2022/</link>
      <pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/chuan_cvpr_2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Action2video: Generating Videos of Human 3D Actions</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/chuan-a2v-ijcv-22/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/chuan-a2v-ijcv-22/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DMRA: Depth-induced Multi-scale Recurrent Attention Network for RGB-D Saliency Detection</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/wei-et-al-tip-22/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/wei-et-al-tip-22/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/wei-et-al-iclr-22/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/wei-et-al-iclr-22/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Investigating Pose Representations and Motion Contexts Modeling for 3D Motion Prediction</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/shuang_investigating_2022/</link>
      <pubDate>Tue, 04 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/shuang_investigating_2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Action2video: Generating Videos of Human 3D Actions&#34; is accepted by International Journal of Computer Vision(IJCV) 2022!</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/chuan_ijcv_2022/</link>
      <pubDate>Tue, 04 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/chuan_ijcv_2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Investigating Pose Representations and Motion Contexts Modeling for 3D Motion Prediction&#34; is accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)!</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/shuang_tpami2022/</link>
      <pubDate>Tue, 04 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/shuang_tpami2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Joint Semantic Mining for Weakly Supervised RGB-D Salient Object Detection</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/jingjing_joint_2021/</link>
      <pubDate>Mon, 06 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/jingjing_joint_2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Joint Semantic Mining for Weakly Supervised RGB-D Salient Object Detection&#34; is accepted by NeurIPS 2021</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/jingjing_neurips2021/</link>
      <pubDate>Mon, 06 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/jingjing_neurips2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>3D Pose Estimation and Future Motion Prediction from 2D Images</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/ji_3d_2021/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/ji_3d_2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;3D pose estimation and future motion prediction from 2D images&#34; is accepted by Pattern Recogntion</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/ji_pr_2021/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/ji_pr_2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/mojtaba_chase_2021/</link>
      <pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/mojtaba_chase_2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Enhancing Human Motion Assessment by Self-supervised Representation Learning</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/mahdiar_enhancing_2021/</link>
      <pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/mahdiar_enhancing_2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search&#34; is accepted by BMVC 2021</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/mojtaba_bmvc2021/</link>
      <pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/mojtaba_bmvc2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Enhancing Human Motion Assessment by Self-supervised Representation Learning&#34; is accepted by BMVC 2021</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/mahdiar_bmvc2021/</link>
      <pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/mahdiar_bmvc2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Automated Generation of Accurate and Fluent Medical X-ray Reports</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/hoang_automated_emnlp/</link>
      <pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/hoang_automated_emnlp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>EventHPE: Event-based 3D Human Pose and Shape Estimation</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/shihao_eventhpe_2021/</link>
      <pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/shihao_eventhpe_2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Joint Visual and Audio Learning for Video Highlight Detection</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/tk_joint_2021/</link>
      <pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/tk_joint_2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Automated Generation of Accurate &amp; Fluent Medical X-ray Reports&#34; is accepted by EMNLP 2021</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/hoang_emnlp2021/</link>
      <pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/hoang_emnlp2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dual Learning Music Composition and Dance Choreography</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/shuangwu_dual_2021/</link>
      <pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/shuangwu_dual_2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Dual Learning Music Composition and Dance Choreography&#34; is accepted by ACM Multimedia 2021</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/shuang_acmmm2021/</link>
      <pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/shuang_acmmm2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;EventHPE: Event-based 3-D Human Pose Estimation&#34; is accepted by ICCV 2021</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/shihao_iccv2021/</link>
      <pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/shihao_iccv2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Joint Visual and Audio Learning for Video Highlight Detection&#34; is accepted by ICCV 2021</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/tk_iccv2021/</link>
      <pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/tk_iccv2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Calibrated RGB-D Salient Object Detection&#34; is accepted by IEEE Conference on Computer Vision and Pattern Recognition 2021</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/wei_cvpr_2021a/</link>
      <pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/wei_cvpr_2021a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Learning Calibrated Medical Image Segmentation via Multi-rater Agreement Modeling&#34; is accepted by IEEE Conference on Computer Vision and Pattern Recognition 2021 (Best Paper Candidate)</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/wei_cvpr_2021b/</link>
      <pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/wei_cvpr_2021b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Calibrated RGB-D Salient Object Detection</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/wei-et-al-cvpr-21/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/wei-et-al-cvpr-21/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning Calibrated Medical Image Segmentation via Multi-rater Agreement Modeling</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/wei-et-al-cvpr-21-mrnet/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/wei-et-al-cvpr-21-mrnet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Stabilizing Training of Generative Adversarial Nets via Langevin Stein Variational Gradient Descent</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/li-et-al-tnnls-22/</link>
      <pubDate>Wed, 30 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/li-et-al-tnnls-22/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Deep Learning for Visual Tracking: A Comprehensive Survey&#34; is accepted by IEEE Transactions on Intelligent Transportation Systems (T-ITS)</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/mojtaba_tits2020/</link>
      <pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/mojtaba_tits2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Action2Motion: Conditioned Generation of 3D Human Motions</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/chuan_action2motion_mm/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/chuan_action2motion_mm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Action2Motion: Conditioned Generation of 3-D Human Motions&#34; is accepted by ACM Multimedia 2020</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/chuan_acmmm2020/</link>
      <pubDate>Sat, 25 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/chuan_acmmm2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;3D Human Shape Reconstruction from a Polarization Image&#34; is accepted by ECCV 2020</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/zou_eccv2020/</link>
      <pubDate>Sat, 04 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/zou_eccv2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>3D Human Shape Reconstruction from a Polarization Image</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/shihao-polarization-2020/</link>
      <pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/shihao-polarization-2020/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>FALCONS: FAst Learner-grader for CONtorted poses in Sports</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/mahdiar_falcons_2020/</link>
      <pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/mahdiar_falcons_2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SparseFusion: Dynamic Human Avatar Modeling from Sparse RGBD Images</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/xinxin_tmm_2020/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/xinxin_tmm_2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>3D Pose Estimation and Future Motion Prediction from 2D Images</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/project/jiyang_multitask_2020/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/project/jiyang_multitask_2020/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Towards Natural and Accurate Future Motion Prediction of Humans and Animals</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/shuangwu_hmr_2019/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/shuangwu_hmr_2019/</guid>
      <description>&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Hierarchical Motion Recurrent Network for Future Motion Prediction of Humans and Animals</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/project/shuangwu_hmr_2019/</link>
      <pubDate>Sat, 27 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/project/shuangwu_hmr_2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://vision-and-learning-lab-ualberta.github.io/archive/opening_postdoc/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/archive/opening_postdoc/</guid>
      <description>&lt;h2 id=&#34;joining-us&#34;&gt;Joining us&amp;hellip;&lt;/h2&gt;
&lt;p&gt;The Vision &amp;amp; Learning Lab at the ECE Dept. University of Alberta, is inviting outstanding Postdoc candidates to join us.&lt;/p&gt;
&lt;p&gt;We are setting up a new lab at the ECE Dept., University of Alberta, focusing on exciting research topics in computer vision and machine learning. We are looking for exceptional Postdocs to join us.&lt;/p&gt;
&lt;p&gt;Further information about the PI, Dr. Li Cheng, can be found below, or via 
&lt;a href=&#34;https://www.ece.ualberta.ca/~lcheng5/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.ece.ualberta.ca/~lcheng5/&lt;/a&gt; or by direct inquiries to 
&lt;a href=&#34;mailto:lcheng5@ualberta.ca&#34;&gt;lcheng5@ualberta.ca&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Potential candidates are requested to email their CVs (in PDF) to Li Cheng (
&lt;a href=&#34;mailto:lcheng5@ualberta.ca&#34;&gt;lcheng5@ualberta.ca&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;about-the-pi&#34;&gt;About the PI&amp;hellip;&lt;/h2&gt;
&lt;p&gt;Li CHENG is an associate professor with the ECE Dept., University of Alberta, Canada. His research expertise is mainly in computer vision and machine learning, with application focus in both visual behavior analysis and biomedical image analysis. His research work has resulted in over 90 referred papers including those published at journals such as IEEE Trans. Pattern Analysis and Machine Intelligence, International Journal of Computer Vision, as well as conferences such as ICML, NIPS, ICCV, CVPR, MICCAI, and AAAI. He is a senior member of IEEE.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
