<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chuan Guo | Vision and Learning Lab @ UAlberta</title>
    <link>https://vision-and-learning-lab-ualberta.github.io/author/chuan-guo/</link>
      <atom:link href="https://vision-and-learning-lab-ualberta.github.io/author/chuan-guo/index.xml" rel="self" type="application/rss+xml" />
    <description>Chuan Guo</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© Vision and Learning Lab, Univerity of Alberta 2024</copyright><lastBuildDate>Tue, 27 Feb 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://vision-and-learning-lab-ualberta.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Chuan Guo</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/author/chuan-guo/</link>
    </image>
    
    <item>
      <title>MoMask: Generative Masked Modeling of 3D Human Motions</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/chuan-cvpr-2024/</link>
      <pubDate>Tue, 27 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/chuan-cvpr-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;MoMask: Generative Masked Modeling of 3D Human Motions &#34; is accepted by IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2024!</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/chuan_cvpr_2024/</link>
      <pubDate>Tue, 27 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/chuan_cvpr_2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Congratulates to Dr. Chuan Guo for successfully defending his PhD thesis! We wish him all the best in his future endeavors and are confident that his work will continue to inspire and lead to significant advancements.</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/chuan_defense_2024/</link>
      <pubDate>Fri, 19 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/chuan_defense_2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Generative Human Motion Stylization in Latent Space</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/chuan-iclr-2024/</link>
      <pubDate>Tue, 16 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/chuan-iclr-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Generative Human Motion Stylization in Latent Space &#34; is accepted by International Conference on Learning Representations (ICLR) 2024!</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/chuan_iclr_2024/</link>
      <pubDate>Tue, 16 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/chuan_iclr_2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Congratulates to Chuan for being awarded the Alberta Innovate Graduate Scholarship!</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/chuan_aigsscholarship_2022/</link>
      <pubDate>Mon, 13 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/chuan_aigsscholarship_2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts.&#34; is accepted by European Conference on Computer Vision (ECCV) 2022!</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/chuan_eccv_2022/</link>
      <pubDate>Fri, 15 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/chuan_eccv_2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts.</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/chuan-tm2t-eccv-2022/</link>
      <pubDate>Fri, 15 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/chuan-tm2t-eccv-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Human Pose and Shape Estimation from Single Polarization Images</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/shihao-et-al-tmm-2022/</link>
      <pubDate>Fri, 25 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/shihao-et-al-tmm-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Generating Diverse and Natural 3D Human Motions from Text&#34; is accepted by IEEE Conference on Computer Vision and Pattern Recognition(CVPR) 2022!</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/chuan_cvpr_2022/</link>
      <pubDate>Mon, 07 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/chuan_cvpr_2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Action2video: Generating Videos of Human 3D Actions</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/chuan-a2v-ijcv-22/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/chuan-a2v-ijcv-22/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/wei-et-al-iclr-22/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/wei-et-al-iclr-22/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Action2video: Generating Videos of Human 3D Actions&#34; is accepted by International Journal of Computer Vision(IJCV) 2022!</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/chuan_ijcv_2022/</link>
      <pubDate>Tue, 04 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/chuan_ijcv_2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>EventHPE: Event-based 3D Human Pose and Shape Estimation</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/shihao_eventhpe_2021/</link>
      <pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/shihao_eventhpe_2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;EventHPE: Event-based 3-D Human Pose Estimation&#34; is accepted by ICCV 2021</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/shihao_iccv2021/</link>
      <pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/shihao_iccv2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Action2Motion: Conditioned Generation of 3D Human Motions</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/publication/chuan_action2motion_mm/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/publication/chuan_action2motion_mm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Our paper &#34;Action2Motion: Conditioned Generation of 3-D Human Motions&#34; is accepted by ACM Multimedia 2020</title>
      <link>https://vision-and-learning-lab-ualberta.github.io/post/chuan_acmmm2020/</link>
      <pubDate>Sat, 25 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://vision-and-learning-lab-ualberta.github.io/post/chuan_acmmm2020/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
